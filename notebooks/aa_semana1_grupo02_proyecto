# -*- coding: utf-8 -*-
"""AA_Semana1_Grupo02_Proyecto.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZgCaVF4wkg4b4zoNKkapJRhapL8fni22

#Materia: APRENDIZAJE AUTOMATICO
#Fecha: 28 Enero 2026
#Estudiantes:
#Ingeniero Gonzalo Mejia Alcivar
#Ingeniero Jorge Ortiz Merchan
"""


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Cargar el dataset
file_path = "/content/drive/MyDrive/AA_MaestriaAI/r_2020_2024.csv"
df = pd.read_csv(
    file_path,
    sep=";",
    encoding="utf-8",
    engine="python",
    on_bad_lines="skip"
)

print(df.shape)
df.head()

# Mostrar las primeras filas del dataset para familiarizarse
print("Primeras 5 filas del dataset:")
print(df.head())

"""## LIMPIEZA DE DATOS"""

# --------------------------
# LIMPIEZA DE DATOS
# --------------------------

# 1. Verificar valores nulos en el dataset
print("Valores nulos en cada columna:")
print(df.isnull().sum())

# Cantidad de registros antes de eliminar nulos
print("Registros antes de eliminar nulos:", df.shape[0])

# Eliminar filas que contengan al menos un valor nulo
df_clean = df.dropna()

# Cantidad de registros despu√©s de eliminar nulos
print("Registros despu√©s de eliminar nulos:", df_clean.shape[0])

print(df.dtypes)

# ==========================
# LIMPIEZA + CONVERSI√ìN DE TIPOS
# ==========================

# Cargamos el dataset original (df) para transformarlo en df_clean
# df = pd.read_csv('DeltaDeDatos.csv') # Aseg√∫rate de tener df cargado
df_clean = df.copy()

# 1) Normalizar nombres de columnas
# Esto elimina espacios, puntos y saltos de l√≠nea para evitar errores en las f√≥rmulas
df_clean.columns = (
    df_clean.columns
    .astype(str)
    .str.strip()
    .str.replace('\n', '_', regex=False)
    .str.replace(' ', '_', regex=False)
    .str.replace('.', '', regex=False)
)

# 2) Definir columnas por tipo (Basado en tu dataset real)
int_columns = ['Posici√≥n', 'A√±o', 'Cant_Empleados']

float_columns = [
    'Activio_2024',
    'Patrimonio_2024',
    'Ingreso_por_ventas_2024',
    'Utilidad_antes_del_impuesto_2024',
    'Utilidad_del_ejercicio_2024',
    'Utilidad_neta_2024',
    'IR_causado_2024',
    'Ingreso_Total_2024'
]

categorical_columns = [
    'Nombre',
    'Tipo_Compa√±ia',
    'Actividad_econ√≥mica',
    'Regi√≥n',
    'Provincia',
    'Ciudad',
    'Sector'
]

# 3) Convertir enteros (Uso de 'Int64' permite manejar nulos si existieran)
for col in int_columns:
    if col in df_clean.columns:
        df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce').astype('Int64')

# 4) Convertir floats (Se recomienda float32 para ahorrar memoria en 600k filas)
for col in float_columns:
    if col in df_clean.columns:
        df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce').astype('float32')

# 5) Convertir categ√≥ricas (Reduce dr√°sticamente el uso de RAM frente a 'object')
for col in categorical_columns:
    if col in df_clean.columns:
        df_clean[col] = df_clean[col].astype('category')

# 6) Gesti√≥n de Filas: Nulos y Duplicados
print(f"Registros iniciales: {df_clean.shape[0]}")

# Eliminar filas con nulos en columnas cr√≠ticas (evita errores en c√°lculos financieros)
df_clean = df_clean.dropna(subset=['Utilidad_neta_2024', 'Ingreso_Total_2024'])

# Eliminar duplicados exactos
df_clean = df_clean.drop_duplicates()

print(f"Registros tras limpieza: {df_clean.shape[0]}")
print(f"Duplicados restantes: {df_clean.duplicated().sum()}")

# 7) Verificaci√≥n de memoria y tipos
df_clean.info()

"""## AN√ÅLISIS EXPLORATORIO DE DATOS (EDA)"""

# --------------------------
# AN√ÅLISIS EXPLORATORIO DE DATOS (EDA)
# --------------------------

# 1. Descripci√≥n estad√≠stica general de las columnas num√©ricas
print("\nDescripci√≥n estad√≠stica de las columnas num√©ricas:")
print(df_clean.describe())

# 2. Visualizacion de Delta de Datos
# Crea una variable temporal con las 20 filas y gu√°rdala
file_path = "/content/drive/MyDrive/AA_MaestriaAI/top_20_filas.csv"
df_clean.head(20).to_csv(file_path, index=False)

# 1.- Importacion de Librerias para Proceso de ML
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score

# 2. Creaci√≥n de variables predictoras (Indicadores Financieros)
epsilon = 1e-7  # Evita divisi√≥n para cero

# Margen Bruto (Usando Utilidad antes de impuesto como proxy si no hay utilidad bruta directa)
df_clean['Margen_Bruto'] = df_clean['Utilidad_antes_del_impuesto_2024'] / (df_clean['Ingreso_Total_2024'] + epsilon)

# Margen Neto
df_clean['Margen_Neto'] = df_clean['Utilidad_neta_2024'] / (df_clean['Ingreso_Total_2024'] + epsilon)

# ROA (Return on Assets)
df_clean['ROA'] = df_clean['Utilidad_neta_2024'] / (df_clean['Activio_2024'] + epsilon)

# ROE (Return on Equity)
df_clean['ROE'] = df_clean['Utilidad_neta_2024'] / (df_clean['Patrimonio_2024'] + epsilon)

# Productividad por empleado
df_clean['Productividad_Empleado'] = df_clean['Ingreso_Total_2024'] / (df_clean['Cant_Empleados'] + epsilon)

# Carga tributaria
df_clean['Carga_Tributaria'] = df_clean['IR_causado_2024'] / (df_clean['Ingreso_por_ventas_2024'] + epsilon)

# 3. Creaci√≥n de Variable Objetivo (Target)
# Clasificaci√≥n multiclase en 3 niveles basados en el Margen Neto
df_clean['Desempeno'] = pd.qcut(df_clean['Margen_Neto'], q=3, labels=['Bajo', 'Medio', 'Alto'])

print("Ingenier√≠a de caracter√≠sticas completada.")
print(df_clean['Desempeno'].value_counts(normalize=True))

# 3 Divisi√≥n de Datos y Preprocesamiento

# ==========================================
# INGENIER√çA DE CARACTER√çSTICAS
# ==========================================

epsilon = 1e-7  # Constante para evitar divisi√≥n por cero

# C√°lculo de los 6 Indicadores Predictores (Metodolog√≠a)
# Margen Bruto (Proxy con Utilidad antes de Impuestos)
df_clean['Margen_Bruto'] = df_clean['Utilidad_antes_del_impuesto_2024'] / (df_clean['Ingreso_Total_2024'] + epsilon)

# Margen Neto
df_clean['Margen_Neto'] = df_clean['Utilidad_neta_2024'] / (df_clean['Ingreso_Total_2024'] + epsilon)

# ROA (Rentabilidad sobre Activos)
df_clean['ROA'] = df_clean['Utilidad_neta_2024'] / (df_clean['Activio_2024'] + epsilon)

# ROE (Rentabilidad sobre Patrimonio)
df_clean['ROE'] = df_clean['Utilidad_neta_2024'] / (df_clean['Patrimonio_2024'] + epsilon)

# Productividad por Empleado
df_clean['Productividad_Empleado'] = df_clean['Ingreso_Total_2024'] / (df_clean['Cant_Empleados'] + epsilon)

# Carga Tributaria (IR sobre Ventas)
df_clean['Carga_Tributaria'] = df_clean['IR_causado_2024'] / (df_clean['Ingreso_por_ventas_2024'] + epsilon)

# 2.2) Limpieza de valores extremos generados por el c√°lculo
# Reemplazamos infinitos por NaN y luego eliminamos esas filas si existen
df_clean = df_clean.replace([np.inf, -np.inf], np.nan)
df_clean = df_clean.dropna(subset=['Margen_Neto', 'ROA', 'ROE', 'Productividad_Empleado', 'Carga_Tributaria'])

# 2.3) Creaci√≥n de la Variable Objetivo (Target)
# Clasificamos en 3 niveles de desempe√±o (Bajo, Medio, Alto) basados en el Margen Neto
# Usamos duplicates='drop' para manejar casos donde los l√≠mites de los cuartiles coincidan
df_clean['Desempeno'] = pd.qcut(df_clean['Margen_Neto'], q=3, labels=['Bajo', 'Medio', 'Alto'], duplicates='drop')

print("‚úÖ Ingenier√≠a de caracter√≠sticas completada.")
print("\nDistribuci√≥n porcentual del desempe√±o:")
print(df_clean['Desempeno'].value_counts(normalize=True) * 100)

# Verificaci√≥n de que no queden nulos tras los c√°lculos
print(f"\nTotal de registros para el modelo: {df_clean.shape[0]}")

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OrdinalEncoder
import joblib

# =========================================================
# 4. FILTRADO POR A√ëO Y PREPROCESAMIENTO EFICIENTE
# =========================================================

# 1) Filtrado por A√±o 2024
# Reducimos el dataset inmediatamente para acelerar el entrenamiento
df_modelo = df_clean[df_clean['A√±o'] == 2024].copy()

# Verificaci√≥n de seguridad
if df_modelo.empty:
    print("‚ùå Error: No hay datos para el a√±o 2024. Revisa el dataset o usa el a√±o 2023.")
else:
    print(f"‚úÖ Registros filtrados para el a√±o 2024: {df_modelo.shape[0]}")

    # 2) Selecci√≥n de variables (Features) seg√∫n la metodolog√≠a
    # Nota: Mantenemos los nombres de columnas originales del dataset
    num_features = [
        'Cant_Empleados', 'Activio_2024', 'Patrimonio_2024', 'Ingreso_Total_2024',
        'Margen_Bruto', 'Margen_Neto', 'ROA', 'ROE',
        'Productividad_Empleado', 'Carga_Tributaria'
    ]

    cat_features = ['Regi√≥n', 'Sector']

    # Definir matriz X y vector objetivo y
    X = df_modelo[num_features + cat_features]
    y = df_modelo['Desempeno']

    # 3) Definici√≥n del Preprocesador (Pipeline de transformaci√≥n)
    # StandardScaler para num√©ricos y OrdinalEncoder para categor√≠as
    preprocessor = ColumnTransformer(
        transformers=[
            ('num', StandardScaler(), num_features),
            ('cat', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1), cat_features)
        ]
    )

    # 4) Divisi√≥n Estratificada (Train: 70%, Val: 15%, Test: 15%)
    # Paso A: Separar el 70% para entrenamiento
    X_train, X_temp, y_train, y_temp = train_test_split(
        X, y, test_size=0.30, random_state=42, stratify=y
    )

    # Paso B: Dividir el 30% restante a la mitad (15% val y 15% test)
    X_val, X_test, y_val, y_test = train_test_split(
        X_temp, y_temp, test_size=0.50, random_state=42, stratify=y_temp
    )

    # 5) Transformaci√≥n de los Datos
    # Ajustamos solo con entrenamiento para evitar fuga de datos (Data Leakage)
    X_train_proc = preprocessor.fit_transform(X_train)
    X_val_proc = preprocessor.transform(X_val)
    X_test_proc = preprocessor.transform(X_test)

    # 6) Guardar el Preprocesador
    # Es vital para realizar inferencias con nuevos datos m√°s adelante
    joblib.dump(preprocessor, 'preprocessor_2025.pkl')

    print("\n‚úÖ Preprocesamiento eficiente completado.")
    print(f"--- Distribuci√≥n final de datos ---")
    print(f"Entrenamiento (70%): {X_train_proc.shape[0]} registros")
    print(f"Validaci√≥n    (15%): {X_val_proc.shape[0]} registros")
    print(f"Prueba/Test   (15%): {X_test_proc.shape[0]} registros")



from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import classification_report, f1_score
import joblib

# =========================================================
# 5. ENTRENAMIENTO CON INDICADORES DE AVANCE (%)
# =========================================================

# 4.1 Configuraci√≥n de Modelos con VERBOSE activado
# verbose=2 o verbose=1 nos permite ver el progreso en tiempo real
models = {
    "Random Forest": RandomForestClassifier(
        n_estimators=100,
        max_depth=15,
        n_jobs=-1,
        random_state=42,
        class_weight='balanced',
        verbose=1  # <--- Esto mostrar√° el progreso de construcci√≥n de los √°rboles
    ),
    "Gradient Boosting": GradientBoostingClassifier(
        n_estimators=50,
        learning_rate=0.1,
        max_depth=5,
        random_state=42,
        verbose=1  # <--- Mostrar√° el progreso de cada iteraci√≥n de boosting
    )
}

results = {}

print("üöÄ Iniciando proceso de entrenamiento...")

for name, model in models.items():
    print(f"\n{'='*10} Entrenando {name} {'='*10}")
    # El output aparecer√° justo debajo de la celda mientras entrena
    model.fit(X_train_proc, y_train)

    # Evaluaci√≥n r√°pida
    y_pred = model.predict(X_val_proc)
    f1 = f1_score(y_val, y_pred, average='weighted')
    results[name] = {'modelo': model, 'f1': f1}

    print(f"\n‚úÖ {name} finalizado. F1-Score obtenido: {f1:.4f}")

# 4.2 Selecci√≥n del Mejor Modelo
mejor_nombre = max(results, key=lambda x: results[x]['f1'])
mejor_modelo = results[mejor_nombre]['modelo']

print("\n" + "*"*40)
print(f"üèÜ MEJOR MODELO PARA 2024: {mejor_nombre}")
print("*"*40)

# 4.3 Informe Final de Negocio
print("\nReporte Metodol√≥gico de Clasificaci√≥n:")
print(classification_report(y_val, mejor_modelo.predict(X_val_proc)))

# 4.4 Guardar para Implementaci√≥n
joblib.dump(mejor_modelo, f'mejor_modelo_{mejor_nombre.replace(" ", "_")}_2025.pkl')

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from sklearn.metrics import confusion_matrix, classification_report, f1_score

# =========================================================
# 5. EVALUACI√ìN DE M√âTRICAS Y EXPLICABILIDAD
# =========================================================

# 5.1 Generar predicciones y calcular F1-Score
y_val_pred = mejor_modelo.predict(X_val_proc)
f1_final = f1_score(y_val, y_val_pred, average='weighted')

# 5.2 Visualizaci√≥n de la Matriz de Confusi√≥n (Normalizada)
plt.figure(figsize=(14, 5))

plt.subplot(1, 2, 1)
# Aseguramos el orden de las etiquetas
clases = ['Bajo', 'Medio', 'Alto']
cm_norm = confusion_matrix(y_val, y_val_pred, labels=clases, normalize='true')

sns.heatmap(cm_norm, annot=True, fmt='.2%', cmap='Blues',
            xticklabels=clases,
            yticklabels=clases)
plt.title(f'Matriz de Confusi√≥n Normalizada\n({mejor_nombre})', fontsize=12)
plt.xlabel('Predicci√≥n del Modelo')
plt.ylabel('Realidad')

# 5.3 Heatmap de M√©tricas (Precision, Recall, F1)
plt.subplot(1, 2, 2)
# Corregido: target_names debe coincidir con las clases reales (3)
report_dict = classification_report(y_val, y_val_pred, target_names=clases, output_dict=True)

# Convertimos el reporte a DataFrame y seleccionamos solo las clases para el mapa de calor
report_df = pd.DataFrame(report_dict).iloc[:-1, :3].T # Excluimos m√©tricas globales para el heatmap por clase

sns.heatmap(report_df, annot=True, cmap='RdYlGn', fmt='.2f')
plt.title('M√©tricas Detalladas: Precision, Recall y F1', fontsize=12)
plt.show()

# 5.4 Importancia de Variables (Explicabilidad)
todas_las_features = num_features + cat_features
importancias = mejor_modelo.feature_importances_

feat_importancia_df = pd.DataFrame({
    'Indicador': todas_las_features,
    'Importancia': importancias
}).sort_values(by='Importancia', ascending=True)

plt.figure(figsize=(10, 6))
colors = plt.cm.viridis(feat_importancia_df['Importancia'] / max(feat_importancia_df['Importancia']))
plt.barh(feat_importancia_df['Indicador'], feat_importancia_df['Importancia'], color=colors)
plt.title(f'¬øQu√© indicadores financieros influyen m√°s en el Desempe√±o?', fontsize=13)
plt.xlabel('Poder Predictivo (Importancia)')
plt.grid(axis='x', linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()

# 5.5 Resumen de Evaluaci√≥n T√©cnica (Criterio de √âxito)
print(f"\n" + "="*55)
print(f"üìä RESUMEN DE EVALUACI√ìN FINAL - A√ëO 2024")
print(f"="*55)
print(f"Mejor Modelo Seleccionado: {mejor_nombre}")
print(f"M√©trica F1-Score (Weighted): {f1_final:.4f}")

# Evaluaci√≥n contra tu meta metodol√≥gica
meta = 0.75
if f1_final >= meta:
    print(f"‚úÖ ESTADO: √âXITO T√âCNICO ALCANZADO (Super√≥ la meta de {meta})")
else:
    print(f"‚ö†Ô∏è ESTADO: BAJO LA META (Meta {meta} no alcanzada)")

print("-" * 55)
# Extraer la clase con mejor desempe√±o del diccionario
mejor_clase = report_df['f1-score'].idxmax()
print(f"Indicador con mayor peso: {feat_importancia_df.iloc[-1]['Indicador']}")
print(f"Clase clasificada con mayor precisi√≥n: {mejor_clase}")
print("="*55)

import pandas as pd
import numpy as np

# =========================================================
# 6. INFERENCIA: CLASIFICACI√ìN DE NUEVAS EMPRESAS
# =========================================================

def clasificar_nueva_empresa(datos_dict, modelo_a_usar, preprocesador_a_usar):
    """
    Recibe los datos de una empresa, aplica el preprocesamiento guardado
    y devuelve la categor√≠a de desempe√±o con su probabilidad.
    """
    # 1. Convertir el diccionario a DataFrame
    df_new = pd.DataFrame([datos_dict])

    # 2. Asegurar que las columnas est√©n en el orden correcto que espera el preprocesador
    # Usamos las listas definidas en el Segmento 3
    columnas_ordenadas = num_features + cat_features
    df_new = df_new[columnas_ordenadas]

    # 3. Transformar los datos usando el preprocesador ajustado (Scaler + Encoder)
    # No usamos .fit(), solo .transform()
    data_proc = preprocesador_a_usar.transform(df_new)

    # 4. Realizar la predicci√≥n
    categoria = modelo_a_usar.predict(data_proc)[0]

    # 5. Calcular la probabilidad (Confianza)
    probabilidades = modelo_a_usar.predict_proba(data_proc)
    confianza = np.max(probabilidades)

    return categoria, confianza

# --- EJEMPLO DE PRUEBA REAL ---
# Estos datos simulan la entrada de una empresa para el a√±o 2025
empresa_test = {
    'Cant_Empleados': 120,
    'Activio_2024': 2000000.0,
    'Patrimonio_2024': 1000000.0,
    'Ingreso_Total_2024': 4000000.0,
    'Ingreso_por_ventas_2024': 3800000.0,
    'Margen_Bruto': 0.25,
    'Margen_Neto': 0.10,
    'ROA': 0.08,
    'ROE': 0.20,
    'Productividad_Empleado': 33333.3,
    'Carga_Tributaria': 0.02,
    'Regi√≥n': 'SIERRA',
    'Sector': 'SOCIETARIO'
}

# Ejecutar la inferencia
resultado, score = clasificar_nueva_empresa(empresa_test, mejor_modelo, preprocessor)

print("="*40)
print("üìä RESULTADO DE LA EVALUACI√ìN 2024")
print("="*40)
print(f"Clasificaci√≥n sugerida: {resultado.upper()}")
print(f"Nivel de Confianza:    {score:.2%}")
print("-"*40)

# Interpretaci√≥n para el usuario/negocio
if resultado == 'Alto':
    print("Sugerencia: Empresa con s√≥lidos indicadores. Candidata para inversi√≥n/cr√©dito.")
elif resultado == 'Bajo':
    print("Sugerencia: Se recomienda auditor√≠a o revisi√≥n de liquidez y carga tributaria.")
else:
    print("Sugerencia: Desempe√±o estable dentro del promedio del sector.")
